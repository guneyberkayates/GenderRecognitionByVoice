Training Results

guneyberkay@guneys-air GenderRecognition % python3 train.py 
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 256)               33024     
                                                                 
 dropout (Dropout)           (None, 256)               0         
                                                                 
 dense_1 (Dense)             (None, 256)               65792     
                                                                 
 dropout_1 (Dropout)         (None, 256)               0         
                                                                 
 dense_2 (Dense)             (None, 128)               32896     
                                                                 
 dropout_2 (Dropout)         (None, 128)               0         
                                                                 
 dense_3 (Dense)             (None, 128)               16512     
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_4 (Dense)             (None, 64)                8256      
                                                                 
 dropout_4 (Dropout)         (None, 64)                0         
                                                                 
 dense_5 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 156545 (611.50 KB)
Trainable params: 156545 (611.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/100
848/848 [==============================] - 6s 6ms/step - loss: 0.5519 - accuracy: 0.7650 - val_loss: 0.3976 - val_accuracy: 0.8515
Epoch 2/100
848/848 [==============================] - 5s 6ms/step - loss: 0.4174 - accuracy: 0.8343 - val_loss: 0.3765 - val_accuracy: 0.8588
Epoch 3/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3761 - accuracy: 0.8532 - val_loss: 0.3177 - val_accuracy: 0.8651
Epoch 4/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3616 - accuracy: 0.8613 - val_loss: 0.3069 - val_accuracy: 0.8858
Epoch 5/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3490 - accuracy: 0.8660 - val_loss: 0.2870 - val_accuracy: 0.8861
Epoch 6/100
848/848 [==============================] - 6s 7ms/step - loss: 0.3321 - accuracy: 0.8721 - val_loss: 0.2916 - val_accuracy: 0.8860
Epoch 7/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3263 - accuracy: 0.8747 - val_loss: 0.2781 - val_accuracy: 0.8939
Epoch 8/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3214 - accuracy: 0.8773 - val_loss: 0.2770 - val_accuracy: 0.8923
Epoch 9/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3117 - accuracy: 0.8818 - val_loss: 0.2630 - val_accuracy: 0.8996
Epoch 10/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3081 - accuracy: 0.8831 - val_loss: 0.2562 - val_accuracy: 0.8989
Epoch 11/100
848/848 [==============================] - 5s 6ms/step - loss: 0.3008 - accuracy: 0.8860 - val_loss: 0.2545 - val_accuracy: 0.8976
Epoch 12/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2986 - accuracy: 0.8866 - val_loss: 0.2612 - val_accuracy: 0.8944
Epoch 13/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2941 - accuracy: 0.8894 - val_loss: 0.2639 - val_accuracy: 0.8963
Epoch 14/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2885 - accuracy: 0.8907 - val_loss: 0.2659 - val_accuracy: 0.9034
Epoch 15/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2883 - accuracy: 0.8897 - val_loss: 0.2486 - val_accuracy: 0.9042
Epoch 16/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2820 - accuracy: 0.8923 - val_loss: 0.2416 - val_accuracy: 0.9066
Epoch 17/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2854 - accuracy: 0.8930 - val_loss: 0.2533 - val_accuracy: 0.9029
Epoch 18/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2776 - accuracy: 0.8956 - val_loss: 0.2598 - val_accuracy: 0.9016
Epoch 19/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2770 - accuracy: 0.8971 - val_loss: 0.2439 - val_accuracy: 0.9079
Epoch 20/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2755 - accuracy: 0.8981 - val_loss: 0.2622 - val_accuracy: 0.8936
Epoch 21/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2760 - accuracy: 0.8968 - val_loss: 0.2390 - val_accuracy: 0.9159
Epoch 22/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2770 - accuracy: 0.8972 - val_loss: 0.2359 - val_accuracy: 0.9114
Epoch 23/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2734 - accuracy: 0.8971 - val_loss: 0.2432 - val_accuracy: 0.9074
Epoch 24/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2707 - accuracy: 0.9004 - val_loss: 0.2373 - val_accuracy: 0.9112
Epoch 25/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2690 - accuracy: 0.9011 - val_loss: 0.2448 - val_accuracy: 0.9064
Epoch 26/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2668 - accuracy: 0.9012 - val_loss: 0.2272 - val_accuracy: 0.9110
Epoch 27/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2688 - accuracy: 0.9003 - val_loss: 0.2362 - val_accuracy: 0.9112
Epoch 28/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2623 - accuracy: 0.9026 - val_loss: 0.2354 - val_accuracy: 0.9130
Epoch 29/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2608 - accuracy: 0.9026 - val_loss: 0.2339 - val_accuracy: 0.9134
Epoch 30/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2585 - accuracy: 0.9031 - val_loss: 0.2333 - val_accuracy: 0.9100
Epoch 31/100
848/848 [==============================] - 5s 6ms/step - loss: 0.2647 - accuracy: 0.9021 - val_loss: 0.2303 - val_accuracy: 0.9172
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Evaluating the model using 6694 samples...
Loss: 0.2283
Accuracy: 91.26%
210/210 [==============================] - 0s 2ms/step
Precision: 0.9246
Recall: 0.8924
F1 Score: 0.9082

----------------------------------------------------------|||||||||||----------------------------------------------------------
----------------------------------------------------------|||||||||||----------------------------------------------------------

Testing Results


guneyberkay@guneys-air GenderRecognition % python3 test.py
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 256)               33024     
                                                                 
 dropout (Dropout)           (None, 256)               0         
                                                                 
 dense_1 (Dense)             (None, 256)               65792     
                                                                 
 dropout_1 (Dropout)         (None, 256)               0         
                                                                 
 dense_2 (Dense)             (None, 128)               32896     
                                                                 
 dropout_2 (Dropout)         (None, 128)               0         
                                                                 
 dense_3 (Dense)             (None, 128)               16512     
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_4 (Dense)             (None, 64)                8256      
                                                                 
 dropout_4 (Dropout)         (None, 64)                0         
                                                                 
 dense_5 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 156545 (611.50 KB)
Trainable params: 156545 (611.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Please talk
1/1 [==============================] - 0s 126ms/step
Result: male
Probabilities:     Male: 99.67%    Female: 0.33%




#/
if __name__ == "__main__":
    # load the saved model (after training)
    # model = pickle.load(open("result/mlp_classifier.model", "rb"))
    from utils import load_data, split_data, create_model
    import argparse
    parser = argparse.ArgumentParser(description="""Gender recognition script, this will load the model you trained, 
                                    and perform inference on a sample you provide (either using your voice or a file)""")
    parser.add_argument("-f", "--file", help="The path to the file, preferred to be in WAV format")
    args = parser.parse_args()
    file = args.file
    # construct the model
    model = create_model()
    # load the saved/trained weights
    model.load_weights("results/model.h5")
    if not file or not os.path.isfile(file):
        # if file not provided, or it doesn't exist, use your voice
        print("Please talk")
        # put the file name here
        file = "test.wav"
        # record the file (start talking)
        record_to_file(file)
    # extract features and reshape it
    features = extract_feature(file, mel=True).reshape(1, -1)
    # predict the gender!
    male_prob = model.predict(features)[0][0]
    female_prob = 1 - male_prob
    gender = "male" if male_prob > female_prob else "female"
    # show the result!
    print("Result:", gender)
    print(f"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%")
    